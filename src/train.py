import os
import glob
import cv2
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torch.utils.tensorboard import SummaryWriter
import albumentations as A
from albumentations.pytorch import ToTensorV2
from PIL import Image
from tqdm import tqdm
import csv
import time
import datetime
from model import UNet 

# ==========================================
# 1. ä¿®å¤åçš„ Dataset ç±»
# ==========================================
class DriveDataset(Dataset):
    def __init__(self, root_path, mode="train"):
        self.path = root_path
        self.mode = mode
        
        data_folder = "training" if mode == "train" else "test"
        
        self.img_list = sorted(glob.glob(os.path.join(self.path, data_folder, 'images/*.tif')))
        self.mask_list = sorted(glob.glob(os.path.join(self.path, data_folder, '1st_manual/*.gif')))

        # æ•°æ®åˆ‡åˆ†é€»è¾‘
        if mode == "val":
            self.img_list = self.img_list[:5]
            self.mask_list = self.mask_list[:5]
        elif mode == "test":
            self.img_list = self.img_list[5:]
            self.mask_list = self.mask_list[5:]
        # train æ¨¡å¼ä½¿ç”¨å…¨éƒ¨ training æ–‡ä»¶å¤¹æ•°æ®

        # è®¾ç½®é‡å¤å€æ•° (ä»…è®­ç»ƒé›†)
        self.repeat = 50 if mode == "train" else 1

        # --- Transform é…ç½® ---
        if mode == "train":
            self.transform = A.Compose([
                A.RandomCrop(height=128, width=128),
                A.HorizontalFlip(p=0.5),
                A.VerticalFlip(p=0.5),
                A.RandomRotate90(p=0.5),
                A.ElasticTransform(alpha=1, sigma=50, p=0.5),
                A.Normalize(mean=(0.5,), std=(0.5,)),
                ToTensorV2()
            ])
        else:
            # ã€é‡è¦ä¿®å¤ã€‘éªŒè¯é›†ä¸è£å‰ªï¼Œä½†å¿…é¡» Pad åˆ° 32 çš„å€æ•°
            # å¦åˆ™ UNet ä¸‹é‡‡æ · 4 æ¬¡åå†ä¸Šé‡‡æ ·ï¼Œå°ºå¯¸ä¼šå¯¹åº”ä¸ä¸Š
            self.transform = A.Compose([
                A.PadIfNeeded(min_height=None, min_width=None, pad_height_divisor=32, pad_width_divisor=32),
                A.Normalize(mean=(0.5,), std=(0.5,)),
                ToTensorV2()
            ])

    def __len__(self):
        return len(self.img_list) * self.repeat

    def __getitem__(self, index):
        index = index % len(self.img_list)
        
        # è¯»å–å›¾ç‰‡
        img = cv2.imread(self.img_list[index])
        
        # ã€ä¼˜åŒ–ã€‘æå–ç»¿è‰²é€šé“ (Green Channel)ï¼Œè¡€ç®¡å¯¹æ¯”åº¦æœ€é«˜
        # OpenCV æ˜¯ BGR æ ¼å¼ï¼Œæ‰€ä»¥ G é€šé“æ˜¯ index 1
        img = img[:, :, 1] 
        
        # è¯»å– Mask
        mask = np.array(Image.open(self.mask_list[index]))
        mask = (mask > 0).astype(np.float32) 

        # åº”ç”¨å˜æ¢
        augmented = self.transform(image=img, mask=mask)
        img = augmented['image'] # Tensor: [H, W] (å› ä¸ºæ˜¯ç°åº¦è¾“å…¥)
        mask = augmented['mask'] # Tensor: [H, W]
        
        # ã€é‡è¦ä¿®å¤ã€‘æ‰‹åŠ¨å¢åŠ  Channel ç»´åº¦ [H, W] -> [1, H, W]
        # å·ç§¯å±‚éœ€è¦ (Batch, Channel, H, W)
        if img.ndim == 2:
            img = img.unsqueeze(0)
        if mask.ndim == 2:
            mask = mask.unsqueeze(0)

        return img, mask

# ==========================================
# 2. Loss å‡½æ•°
# ==========================================
class DiceLoss(nn.Module):
    def __init__(self):
        super(DiceLoss, self).__init__()

    def forward(self, inputs, targets, smooth=1):
        # inputs å¿…é¡»æ˜¯å·²ç»ç»è¿‡ Sigmoid çš„æ¦‚ç‡å€¼ (0-1)
        inputs = inputs.view(-1)
        targets = targets.view(-1)
        intersection = (inputs * targets).sum()                            
        dice = (2.*intersection + smooth) / (inputs.sum() + targets.sum() + smooth)  
        return 1 - dice

# ==========================================
# 3. è®­ç»ƒä¸»ç¨‹åº
# ==========================================
def train_model():
    # --- é…ç½®å‚æ•° ---
    DATA_PATH = './data/DRIVE'
    BATCH_SIZE = 32       # æ˜¾å­˜å¦‚æœä¸å¤Ÿï¼Œæ”¹ä¸º 8 æˆ– 4
    LEARNING_RATE = 1e-3  # ã€å»ºè®®ã€‘é™ä½å­¦ä¹ ç‡ï¼Œ1e-3 å®¹æ˜“éœ‡è¡
    EPOCHS = 50
    PATIENCE = 5         # æ—©åœè€å¿ƒå€¼
    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # ç»“æœç›®å½•
    current_time = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
    RESULT_DIR = os.path.join('./results', f'exp_{current_time}')
    CHECKPOINT_DIR = os.path.join(RESULT_DIR, 'checkpoints')
    LOG_CSV_PATH = os.path.join(RESULT_DIR, 'training_log.csv')
    
    os.makedirs(CHECKPOINT_DIR, exist_ok=True)
    os.makedirs(os.path.join(RESULT_DIR, 'tensorboard_logs'), exist_ok=True)

    # åˆå§‹åŒ– CSV
    with open(LOG_CSV_PATH, mode='w', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(['Epoch', 'Train Loss', 'Val Loss', 'Val Dice', 'LR', 'Time(s)'])

    # --- æ•°æ®åŠ è½½ ---
    train_ds = DriveDataset(root_path=DATA_PATH, mode="train")
    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)

    val_ds = DriveDataset(root_path=DATA_PATH, mode="val")
    # éªŒè¯é›† Batch Size å¿…é¡»ä¸º 1 (å› ä¸ºå›¾ç‰‡æ²¡åš Cropï¼ŒåŸå§‹å°ºå¯¸å¤§)
    val_loader = DataLoader(val_ds, batch_size=1, shuffle=False)

    # --- æ¨¡å‹ä¸ä¼˜åŒ–å™¨ ---
    model = UNet(n_channels=1, n_classes=1).to(DEVICE)
    
    # ã€é‡è¦ä¿®å¤ã€‘ä½¿ç”¨ BCEWithLogitsLoss (è‡ªå¸¦ Sigmoidï¼Œæ›´ç¨³å®š)
    criterion_bce = nn.BCEWithLogitsLoss() 
    criterion_dice = DiceLoss()
    
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)
    
    writer = SummaryWriter(log_dir=os.path.join(RESULT_DIR, 'tensorboard_logs'))
    
    print(f"ğŸš€ å¼€å§‹è®­ç»ƒ... è®¾å¤‡: {DEVICE} | è®­ç»ƒé›†æ•°é‡: {len(train_ds)}")
    start_time = time.time()
    
    best_val_loss = float('inf')
    early_stop_counter = 0

    # --- è®­ç»ƒå¾ªç¯ ---
    for epoch in range(EPOCHS):
        epoch_start = time.time()
        
        # >>>>>> è®­ç»ƒé˜¶æ®µ <<<<<<
        model.train()
        train_loss = 0.0
        
        train_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{EPOCHS} [Train]")
        for imgs, masks in train_bar:
            imgs = imgs.to(DEVICE)
            masks = masks.to(DEVICE)

            # 1. å‰å‘ä¼ æ’­ (å¾—åˆ° Logitsï¼Œæœªç»è¿‡ Sigmoid)
            preds_logits = model(imgs)

            # 2. è®¡ç®— Loss
            # BCE ç›´æ¥åƒ Logits
            loss_bce = criterion_bce(preds_logits, masks)
            
            # Dice éœ€è¦åƒæ¦‚ç‡ (0-1)ï¼Œæ‰€ä»¥è¿™é‡Œæ‰‹åŠ¨ Sigmoid
            preds_probs = torch.sigmoid(preds_logits)
            loss_dice = criterion_dice(preds_probs, masks)

            # ç»„åˆ Loss (ä½ å¯ä»¥è°ƒæ•´æƒé‡)
            loss = 0.5 * loss_bce + 1.5 * loss_dice

            # 3. åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            train_bar.set_postfix(loss=loss.item(), bce=loss_bce.item(), dice=loss_dice.item())

        avg_train_loss = train_loss / len(train_loader)

        # >>>>>> éªŒè¯é˜¶æ®µ <<<<<<
        model.eval()
        val_loss = 0.0
        val_dice_score = 0.0
        
        with torch.no_grad():
            for imgs, masks in val_loader:
                imgs = imgs.to(DEVICE)
                masks = masks.to(DEVICE)
                
                # Forward
                preds_logits = model(imgs)
                preds_probs = torch.sigmoid(preds_logits)
                
                # Loss
                v_loss_bce = criterion_bce(preds_logits, masks)
                v_loss_dice = criterion_dice(preds_probs, masks)
                
                total_v_loss = 0.5 * v_loss_bce + 1.5 * v_loss_dice
                val_loss += total_v_loss.item()
                
                # è®°å½• Dice Score (1 - DiceLoss) ç”¨äºç›´è§‚å±•ç¤º
                val_dice_score += (1 - v_loss_dice.item())

        avg_val_loss = val_loss / len(val_loader)
        avg_val_dice = val_dice_score / len(val_loader)
        
        # æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler.step(avg_val_loss)
        current_lr = optimizer.param_groups[0]['lr']
        
        epoch_duration = time.time() - epoch_start
        
        # >>>>>> æ—¥å¿—ä¸ä¿å­˜ <<<<<<
        print(f"Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val Dice: {avg_val_dice:.4f} | LR: {current_lr:.1e}")

        writer.add_scalar('Loss/Train', avg_train_loss, epoch)
        writer.add_scalar('Loss/Val', avg_val_loss, epoch)
        writer.add_scalar('Metric/Dice', avg_val_dice, epoch)

        with open(LOG_CSV_PATH, mode='a', newline='') as f:
            csv.writer(f).writerow([epoch+1, avg_train_loss, avg_val_loss, avg_val_dice, current_lr, epoch_duration])

        # æ—©åœä¸æœ€ä½³æ¨¡å‹ä¿å­˜
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            early_stop_counter = 0
            torch.save(model.state_dict(), os.path.join(CHECKPOINT_DIR, "best_model.pth"))
            print(f"âœ… æ¨¡å‹å·²ä¿å­˜ (Best Loss: {best_val_loss:.4f})")
        else:
            early_stop_counter += 1
            print(f"â³ Loss æœªä¸‹é™ ({early_stop_counter}/{PATIENCE})")
            
            if early_stop_counter >= PATIENCE:
                print("ğŸ›‘ è§¦å‘æ—©åœæœºåˆ¶ï¼Œè®­ç»ƒç»“æŸã€‚")
                break

    writer.close()
    print("Training Completed.")

if __name__ == "__main__":
    train_model()